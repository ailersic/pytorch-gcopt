{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '/media/sdb1/home/andrew/gaussian-continuation/config/burgers_config.yaml'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 26\u001b[0m\n\u001b[1;32m     16\u001b[0m config_name \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdefault\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     17\u001b[0m pipe \u001b[38;5;241m=\u001b[39m ConfigPipeline(\n\u001b[1;32m     18\u001b[0m     [\n\u001b[1;32m     19\u001b[0m         YamlConfig(\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     24\u001b[0m     ]\n\u001b[1;32m     25\u001b[0m )\n\u001b[0;32m---> 26\u001b[0m config \u001b[38;5;241m=\u001b[39m \u001b[43mpipe\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_conf\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     27\u001b[0m config_name \u001b[38;5;241m=\u001b[39m pipe\u001b[38;5;241m.\u001b[39msteps[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]\u001b[38;5;241m.\u001b[39mconfig_name\n\u001b[1;32m     29\u001b[0m \u001b[38;5;66;03m# Set-up distributed communication, if using\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/gcenv/lib/python3.11/site-packages/configmypy/pipeline_config.py:19\u001b[0m, in \u001b[0;36mConfigPipeline.read_conf\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     17\u001b[0m kwargs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mdict\u001b[39m()\n\u001b[1;32m     18\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m step \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msteps:\n\u001b[0;32m---> 19\u001b[0m     config, kwargs \u001b[38;5;241m=\u001b[39m \u001b[43mstep\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_conf\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     21\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig \u001b[38;5;241m=\u001b[39m config\n\u001b[1;32m     23\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m config\n",
      "File \u001b[0;32m~/miniconda3/envs/gcenv/lib/python3.11/site-packages/configmypy/yaml_config.py:66\u001b[0m, in \u001b[0;36mYamlConfig.read_conf\u001b[0;34m(self, config, config_file, config_name, config_folder)\u001b[0m\n\u001b[1;32m     64\u001b[0m \u001b[38;5;66;03m# Read the conf\u001b[39;00m\n\u001b[1;32m     65\u001b[0m yaml\u001b[38;5;241m=\u001b[39mYAML()\n\u001b[0;32m---> 66\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mfilepath\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mr\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[1;32m     67\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig \u001b[38;5;241m=\u001b[39m yaml\u001b[38;5;241m.\u001b[39mload(f)\n\u001b[1;32m     69\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m config_name \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/media/sdb1/home/andrew/gaussian-continuation/config/burgers_config.yaml'"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import torch\n",
    "import wandb\n",
    "from configmypy import ConfigPipeline, YamlConfig, ArgparseConfig\n",
    "from torch.nn.parallel import DistributedDataParallel as DDP\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from neuralop import H1Loss, LpLoss, BurgersEqnLoss, ICLoss, WeightedSumLoss, Trainer, get_model\n",
    "from neuralop.datasets import load_burgers_1dtime\n",
    "from neuralop.datasets.data_transforms import MGPatchingDataProcessor\n",
    "from neuralop.training import setup, BasicLoggerCallback\n",
    "from neuralop.utils import get_wandb_api_key, count_model_params\n",
    "\n",
    "\n",
    "# Load the Burgers dataset\n",
    "train_loader, test_loaders, output_encoder = load_burgers_1dtime(data_path=config.data.folder,\n",
    "        n_train=config.data.n_train, batch_size=config.data.batch_size, \n",
    "        n_test=config.data.n_tests[0], batch_size_test=config.data.test_batch_sizes[0],\n",
    "        temporal_length=config.data.temporal_length, spatial_length=config.data.spatial_length,\n",
    "        pad=config.data.get(\"pad\", 0), temporal_subsample=config.data.get(\"temporal_subsample\", 1),\n",
    "        spatial_subsample=config.data.get(\"spatial_subsample\", 1),\n",
    "        )\n",
    "\n",
    "model = get_model(config)\n",
    "model = model.to(device)\n",
    "\n",
    "# Use distributed data parallel\n",
    "if config.distributed.use_distributed:\n",
    "    model = DDP(\n",
    "        model, device_ids=[device.index], output_device=device.index, static_graph=True\n",
    "    )\n",
    "\n",
    "# Create the optimizer\n",
    "optimizer = torch.optim.Adam(\n",
    "    model.parameters(),\n",
    "    lr=config.opt.learning_rate,\n",
    "    weight_decay=config.opt.weight_decay,\n",
    ")\n",
    "\n",
    "if config.opt.scheduler == \"ReduceLROnPlateau\":\n",
    "    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
    "        optimizer,\n",
    "        factor=config.opt.gamma,\n",
    "        patience=config.opt.scheduler_patience,\n",
    "        mode=\"min\",\n",
    "    )\n",
    "elif config.opt.scheduler == \"CosineAnnealingLR\":\n",
    "    scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(\n",
    "        optimizer, T_max=config.opt.scheduler_T_max\n",
    "    )\n",
    "elif config.opt.scheduler == \"StepLR\":\n",
    "    scheduler = torch.optim.lr_scheduler.StepLR(\n",
    "        optimizer, step_size=config.opt.step_size, gamma=config.opt.gamma\n",
    "    )\n",
    "else:\n",
    "    raise ValueError(f\"Got scheduler={config.opt.scheduler}\")\n",
    "\n",
    "\n",
    "# Creating the losses\n",
    "l2loss = LpLoss(d=2, p=2)\n",
    "h1loss = H1Loss(d=2)\n",
    "ic_loss = ICLoss()\n",
    "equation_loss = BurgersEqnLoss(method=config.opt.get('pino_method', None), \n",
    "                               visc=0.01, loss=F.mse_loss)\n",
    "\n",
    "training_loss = config.opt.training_loss\n",
    "if not isinstance(training_loss, (tuple, list)):\n",
    "    training_loss = [training_loss]\n",
    "\n",
    "losses = []\n",
    "weights = []\n",
    "for loss in training_loss:\n",
    "    # Append loss\n",
    "    if loss == 'l2':\n",
    "        losses.append(l2loss)\n",
    "    elif loss == 'h1':\n",
    "        losses.append(h1loss)\n",
    "    elif loss == 'equation':\n",
    "        losses.append(equation_loss)\n",
    "    elif loss == 'ic':\n",
    "        losses.append(ic_loss)\n",
    "    else:\n",
    "        raise ValueError(f'Training_loss={loss} is not supported.')\n",
    "\n",
    "    # Append loss weight\n",
    "    if \"loss_weights\" in config.opt:\n",
    "        weights.append(config.opt.loss_weights.get(loss, 1.))\n",
    "    else:\n",
    "        weights.append(1.)\n",
    "\n",
    "train_loss = WeightedSumLoss(losses=losses, weights=weights)\n",
    "eval_losses = {\"h1\": h1loss, \"l2\": l2loss}\n",
    "\n",
    "if config.verbose:\n",
    "    print(\"\\n### MODEL ###\\n\", model)\n",
    "    print(\"\\n### OPTIMIZER ###\\n\", optimizer)\n",
    "    print(\"\\n### SCHEDULER ###\\n\", scheduler)\n",
    "    print(\"\\n### LOSSES ###\")\n",
    "    print(f\"\\n * Train: {train_loss}\")\n",
    "    print(f\"\\n * Test: {eval_losses}\")\n",
    "    print(f\"\\n### Beginning Training...\\n\")\n",
    "    sys.stdout.flush()\n",
    "\n",
    "# only perform MG patching if config patching levels > 0\n",
    "\n",
    "callbacks = [\n",
    "    BasicLoggerCallback(wandb_init_args)\n",
    "]\n",
    "\n",
    "data_processor = MGPatchingDataProcessor(model=model,\n",
    "                                       levels=config.patching.levels,\n",
    "                                       padding_fraction=config.patching.padding,\n",
    "                                       stitching=config.patching.stitching,\n",
    "                                       device=device,\n",
    "                                       in_normalizer=output_encoder,\n",
    "                                       out_normalizer=output_encoder)\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    n_epochs=config.opt.n_epochs,\n",
    "    data_processor=data_processor,\n",
    "    device=device,\n",
    "    amp_autocast=config.opt.amp_autocast,\n",
    "    callbacks=callbacks,\n",
    "    log_test_interval=config.wandb.log_test_interval,\n",
    "    log_output=config.wandb.log_output,\n",
    "    use_distributed=config.distributed.use_distributed,\n",
    "    verbose=config.verbose,\n",
    "    wandb_log = config.wandb.log\n",
    ")\n",
    "\n",
    "# Log parameter count\n",
    "if is_logger:\n",
    "    n_params = count_model_params(model)\n",
    "\n",
    "    if config.verbose:\n",
    "        print(f\"\\nn_params: {n_params}\")\n",
    "        sys.stdout.flush()\n",
    "\n",
    "    if config.wandb.log:\n",
    "        to_log = {\"n_params\": n_params}\n",
    "        if config.n_params_baseline is not None:\n",
    "            to_log[\"n_params_baseline\"] = (config.n_params_baseline,)\n",
    "            to_log[\"compression_ratio\"] = (config.n_params_baseline / n_params,)\n",
    "            to_log[\"space_savings\"] = 1 - (n_params / config.n_params_baseline)\n",
    "        wandb.log(to_log)\n",
    "        wandb.watch(model)\n",
    "\n",
    "\n",
    "trainer.train(\n",
    "    train_loader,\n",
    "    test_loaders,\n",
    "    optimizer,\n",
    "    scheduler,\n",
    "    regularizer=False,\n",
    "    training_loss=train_loss,\n",
    "    eval_losses=eval_losses,\n",
    ")\n",
    "\n",
    "if config.wandb.log and is_logger:\n",
    "    wandb.finish()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gcenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
