{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import pytorch_lightning as pl\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from gc_celeba_gan import *\n",
    "\n",
    "from torchvision.datasets import CelebA\n",
    "from torchvision import transforms, models\n",
    "from torchmetrics.image.fid import FrechetInceptionDistance\n",
    "\n",
    "print(torch.cuda.is_available())\n",
    "torch.zeros(1).cuda()\n",
    "torch.set_float32_matmul_precision('high')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "162770\n",
      "loading real data into FID\n",
      "0\n",
      "done\n",
      "Files already downloaded and verified\n",
      "model ready!\n",
      "0\n",
      "tensor(1.4160)\n",
      "Files already downloaded and verified\n",
      "model ready!\n",
      "0\n",
      "tensor(0.8456)\n",
      "Files already downloaded and verified\n",
      "model ready!\n",
      "0\n",
      "tensor(0.7179)\n",
      "Files already downloaded and verified\n",
      "model ready!\n",
      "0\n",
      "tensor(1.6320)\n",
      "Files already downloaded and verified\n",
      "model ready!\n",
      "0\n",
      "tensor(0.7010)\n"
     ]
    }
   ],
   "source": [
    "paths = [f'out_CelebAGAN_20_nan_nan_0.0100_0.0000_0.0000/version_{i}/checkpoints/epoch=19-step=76320.ckpt' for i in range(5)]\n",
    "\n",
    "fid = FrechetInceptionDistance(feature=64, reset_real_features=False, normalize=True)\n",
    "\n",
    "transformlist = transforms.Compose([\n",
    "    transforms.Resize(size=64),\n",
    "    transforms.CenterCrop(size=(64, 64)),\n",
    "    transforms.ToTensor(),\n",
    "])\n",
    "dataset = CelebA(os.getcwd(), download=True, target_type='attr', transform=transformlist)\n",
    "dataloader = DataLoader(dataset, batch_size=128, num_workers=48, persistent_workers=True)\n",
    "\n",
    "print(len(dataset))\n",
    "\n",
    "print('loading real data into FID')\n",
    "n = 0\n",
    "for img, _ in dataloader:\n",
    "    print(n)\n",
    "    fid.update(img, real=True)\n",
    "    n += 128\n",
    "    if n == 128:\n",
    "        break\n",
    "print('done')\n",
    "\n",
    "for path in paths:\n",
    "    model = CelebAGAN(float('nan'), float('nan'), 0.0, 0.0, 0)\n",
    "    checkpoint = torch.load(path)\n",
    "    #del checkpoint[\"state_dict\"][\"logcontvar\"]\n",
    "    model.load_state_dict(checkpoint[\"state_dict\"])\n",
    "    model.eval()\n",
    "    print('model ready!')\n",
    "\n",
    "    fid.reset()\n",
    "    n = 0\n",
    "    while n < 128:\n",
    "        print(n)\n",
    "        imgs = model.sample_G(128).detach()\n",
    "        fid.update(imgs, real=False)\n",
    "        n += 128\n",
    "    print(fid.compute())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gcenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
